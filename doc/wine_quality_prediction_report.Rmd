---
title: "Predicting Wine Quality Using Physicochemical Properties"

author: "Contributors: Jianru Deng, Yuyan Guo, Vignesh Lakshmi Rajakumar, Cameron Harris"

date: December 12th, 2020
always_allow_html: true
output: 
  html_document:
    toc: true
bibliography: wine_refs.bib
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
knitr::opts_chunk$set(dpi=300,fig.width=7)
library(tidyverse)
library(knitr)
library(here)
library(kableExtra)

```

## Acknowledgements

The data set was produced by P. Cortez, A. Cerdeira, F. Almeida, T. Matos and J. Reis. Modeling wine preferences by data mining from physicochemical properties. In Decision Support Systems, Elsevier, 47(4):547-553, 2009. It was sourced from [@Dua:2019] Dua, D. and Graff, C. (2019). UCI Machine Learning Repository [<http://archive.ics.uci.edu/ml>]. Irvine, CA: University of California, School of Information and Computer Science.

All machine learning processing and analysis was done using Sci-kit Learn [@sk-learn], python [@Python], R[@R]. Tables and figures in this report were created with the help of the Knitr [@knitr], kableExtra [@kableExtra], docopt[@docopt], pandas [@reback2020pandas] package. File paths were managed using the Here [@here] package.

## Summary

In this project, we aim to build a supervised machine learning pipeline and fit a best performing classification model to predict if a wine is "good"(6 or higher out of 10) or "bad" (5 or lower out of 10) based on its physicochemical properties. After carrying out the model comparison, the best performing model turned out to be the random forest classifier. It performed reasonably on the unseen test set with test accuracy of 0.85. The test set consisted of 1300 examples, and the model correctly predicted 85% of them. Also the test set is fairly large, so the test accuracy can be trusted to be a good approximation of the model performance on the deployment data. Depending on the specific application of the model, the test accuracy can either be good or not. If the downstream application requires higher accuracy, we propose to keep improving the model with more advanced techniques. More suggestions can be found in the last section of the report: "Further Improvements and Potential Problem".

## Introduction of the Data

The [dataset](https://archive.ics.uci.edu/ml/datasets/Wine+Quality) used for this prediction task contains physicochemical properties (features) of nearly 5000 wines and their wine quality as a score from 0-10 (targets). The wine quality scores were determined by human wine taste preference (median of at least 3 evaluations made by wine experts). Each expert graded the wine quality between 0 (very bad) and 10 (excellent). The features (physicochemical properties) were determined by analytic tests. Overall there were eleven numeric features and one categorical feature (see [README.md](https://github.com/UBC-MDS/dsci-522-group14/blob/main/README.md) file for more details on features).

The goal of the project was to answer the following research question:

> Can we predict if a wine is "good" (6 or higher out of 10) or "bad" (5 or lower out of 10) based on its physicochemical properties alone?

It is important to note that the labels "good" and "bad" are purely for classification and were selected by the relative wine quality scores determined by wine experts and the quality boundary identified during the exploratory data analysis (EDA).

This prediction problem could have been framed as either a multi-class classification, binary classification or regression problem. During EDA, the team found two observations that led us to simplify the problem into a binary classification one (i.e. good vs. bad wine). The first observation, which came from a distribution of the training target values, was that not all classes were well represented for the target values. The second observation was that the majority of wine quality scores were between 4-6. From this information, we decided that multi-class classification would yield poor prediction results and provide little in value in identifying important features for determining wine quality. Whereas a binary classification allows us to make better predictions and determine which types of chemical and physical characteristics can be attributed to good wines.

```{r load data, warnings=FALSE, messages=FALSE, echo=FALSE}
file_path <- here("data", "processed", "wine_data.csv")
wine_data <- read_csv(file_path, col_types=cols())

kable(head(wine_data, 10),
      caption = "Table 1. Sample of processed data used for wine quality prediction") %>% 
      kable_styling(full_width = F)

```

Next we analyze the target classes for this data set in the figure below.

```{r, fig.align='center', out.width='15%', fig.cap='Figure 1. Target Class Distribution', echo=FALSE}
knitr::include_graphics(here('results', 'eda_target.png'))
```

For this classification problem we determined that we could ignore class imbalance related intricacies and could begin splitting the data into train and test sets. For this task, an 80/20 train/test split was used.

## Preprocessing of Features

In this analysis there are eleven numeric features and one categorical feature. For this analysis, the numeric features were scaled using a standard scalar transformer, which involves removing the mean and scaling to unit variance [@sk-learn]. The categorical feature, wine type, only contained two values (red and white) and was treated as a binary feature.

## Modelling and Cross Validation Scores

All models were evaluated through five-fold cross validation on the training data set. Accuracy was used as the primary metric to evaluate model performance. For this classification problem, there is low consequence to a false negative or false positive classifications, therefore recall and precision are of low importance to us. Accuracy provides a simple, clear way to compare model performance.

$$ \text{Accuracy} = \frac{\text{Number of correct predictions}}{\text{Number of examples}} $$ The following models were evaluated to predict the wine quality label:

-   Dummy classifier
-   Decision tree
-   Support Vector Classification with Radial Basis Function
-   Logistic Regression
-   Random Forest

```{r model results, echo=FALSE, warnings=FALSE, messages=FALSE}
model_results <- read_csv(here("results", "model_comparison.csv"), col_types = cols())

kable(model_results,
      caption="Table 2. Cross validation scores for models using training dataset") %>% 
      kable_styling(full_width = F)
```

The results in Table 2 show that the Random Forest predicts wine quality with the highest accuracy on the training data with a validation score of `r round(model_results$mean_valid_accuracy[5], 3)`. This was not surprising to our team as Random Forest is one of the most widely used and powerful model for classification problems.

The SVC with RBF model performed the next best with a validation score of `r round(model_results$mean_valid_accuracy[3], 3)`.

## Hyperparameter Optimization

Given the cross validation results above, hyperparameter optimization was carried out on the Random Forest model on the number of trees and maximum tree depth parameters.

```{r hyperparameter results, messages=FALSE, warnings=FALSE, echo=FALSE}
hyperparameter_results <- read_csv(here("results", "hyperparameter_result.csv"), col_types = cols())

kable(hyperparameter_results, caption = "Table 3. Hyperparameter optimization results") %>% 
      kable_styling(full_width = F)

```

The results of optimization two key hyperparameters resulted in slightly improved validation results (note that in Table 3, "test score" is comparable to "valid score" from Table 2).

## Test Data Scores and Conclusions

Finally, with a Random Forest model containing optimized hyperparameters, we were able to test the accuracy of our model on the test data set.

```{r test scores, warnings=FALSE, messages=FALSE, echo=FALSE}
test_scores <- read_csv(here("results", "test_score.csv"), col_types = cols())
```

With optimized hyperparameters, the Random Forest model was able to achieve a test accuracy of `r round(test_scores[[1]], 3)` on the test data set. This is slightly higher than the validation scores using the training data, which tells us that we may have got a bit lucky on the test data set. But overall, the model is doing a decent job at predicting the wine label of "good" or "bad" given the physicochemical properties as features.

Recalling the main research question:

> Can we predict if a wine is "good" (6 or higher out of 10) or "bad" (5 or lower out of 10) based on its physicochemical properties alone?

The results show that with 85% test accuracy, which means that the fitted model is able to predict 85% of the test examples correctly. So it is possible to predict whether a wine may be considered "good" (6/10 or higher) or "bad" (5/10 or lower) given the features we have.

## Further Improvements and Potential Problems

Some further work that may result in higher prediction accuracy and higher model interpretability could include feature selection and feature engineering. Based on the result of EDA, some of the features seem like they could be correlated (e.g. free sulphur dioxide, total sulphur dioxide, sulphates), which may be removed after feature selection. The feature engineering may require some domain knowledge, as the features are very domain-specific. However, adding more relevant features can be potentially helpful.

Except for random forest classifier and all the other classifiers we tested, there are still some other powerful classification models which are not tested in this project but may improve the prediction accuracy. (e.g. XGBoost, LightGBM, Catboost etc)

The original data-set has a quantitative output metric: a rating between 0-10. This problem could be a candidate for a regression model. It would be interesting to compare the effectiveness and usefulness of this consideration and could be explored in a future iteration.

Another point of interest in problem is the subjectivity of wine quality. The current data set uses a median rating from multiple tastings from multiple wine experts as an estimation of quality. While we feel that this estimate is a good enough proxy, it is something to be aware of when using this model.

## References
